{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0dfdf6-b4bd-48f5-bed7-e920cce9464b",
   "metadata": {},
   "source": [
    "# Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23dae92b-3222-456d-b821-2c18dc261d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93521fbf-821c-4352-8183-5fc547fe388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Directories\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "dest_dir = root_dir+'\\\\data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d1600-1747-4ef5-8b80-5e5f37cbeb4c",
   "metadata": {},
   "source": [
    "#### Create Preprocessed Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d04f97-104f-4fac-8b9e-717563de8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(logdir):\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "886fd10c-bb43-49d4-9d9f-7d0773053fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directory(dest_dir)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f1a24-4dde-46ff-84b7-d6d2b8d6a827",
   "metadata": {},
   "source": [
    "### Downloading Data\n",
    "#### Get S&P 500 Constituents List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a7fbfaa-c939-4764-83cf-bb8923e02207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MMM', 'AOS', 'ABT', 'ABBV', 'ABMD', 'ACN', 'ATVI', 'ADM', 'ADBE', 'AAP', 'AMD', 'AES', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE', 'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AMCR', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'ANTM', 'AON', 'APA', 'AAPL', 'AMAT', 'APTV', 'ANET', 'AJG', 'AIZ', 'T', 'ATO', 'ADSK', 'ADP', 'AZO', 'AVB', 'AVY', 'BKR', 'BLL', 'BAC', 'BBWI', 'BAX', 'BDX', 'BRK.B', 'BBY', 'BIO', 'TECH', 'BIIB', 'BLK', 'BK', 'BA', 'BKNG', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BR', 'BRO', 'BF.B', 'CHRW', 'CDNS', 'CZR', 'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CARR', 'CTLT', 'CAT', 'CBOE', 'CBRE', 'CDW', 'CE', 'CNC', 'CNP', 'CDAY', 'CERN', 'CF', 'CRL', 'SCHW', 'CHTR', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CTXS', 'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP', 'ED', 'STZ', 'CPRT', 'GLW', 'CTVA', 'COST', 'CTRA', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DAL', 'XRAY', 'DVN', 'DXCM', 'FANG', 'DLR', 'DFS', 'DISCA', 'DISCK', 'DISH', 'DG', 'DLTR', 'D', 'DPZ', 'DOV', 'DOW', 'DTE', 'DUK', 'DRE', 'DD', 'DXC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'LLY', 'EMR', 'ENPH', 'ETR', 'EOG', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ETSY', 'RE', 'EVRG', 'ES', 'EXC', 'EXPE', 'EXPD', 'EXR', 'XOM', 'FFIV', 'FB', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FRC', 'FE', 'FISV', 'FLT', 'FMC', 'F', 'FTNT', 'FTV', 'FBHS', 'FOXA', 'FOX', 'BEN', 'FCX', 'GPS', 'GRMN', 'IT', 'GNRC', 'GD', 'GE', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GL', 'GS', 'HAL', 'HBI', 'HAS', 'HCA', 'PEAK', 'HSIC', 'HES', 'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HWM', 'HPQ', 'HUM', 'HBAN', 'HII', 'IBM', 'IEX', 'IDXX', 'INFO', 'ITW', 'ILMN', 'INCY', 'IR', 'INTC', 'ICE', 'IFF', 'IP', 'IPG', 'INTU', 'ISRG', 'IVZ', 'IPGP', 'IQV', 'IRM', 'JBHT', 'JKHY', 'J', 'SJM', 'JNJ', 'JCI', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI', 'KLAC', 'KHC', 'KR', 'LHX', 'LH', 'LRCX', 'LW', 'LVS', 'LEG', 'LDOS', 'LEN', 'LNC', 'LIN', 'LYV', 'LKQ', 'LMT', 'L', 'LOW', 'LUMN', 'LYB', 'MTB', 'MRO', 'MPC', 'MKTX', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MTCH', 'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'MET', 'MTD', 'MGM', 'MCHP', 'MU', 'MSFT', 'MAA', 'MRNA', 'MHK', 'TAP', 'MDLZ', 'MPWR', 'MNST', 'MCO', 'MS', 'MSI', 'MSCI', 'NDAQ', 'NTAP', 'NFLX', 'NWL', 'NEM', 'NWSA', 'NWS', 'NEE', 'NLSN', 'NKE', 'NI', 'NSC', 'NTRS', 'NOC', 'NLOK', 'NCLH', 'NRG', 'NUE', 'NVDA', 'NVR', 'NXPI', 'ORLY', 'OXY', 'ODFL', 'OMC', 'OKE', 'ORCL', 'OGN', 'OTIS', 'PCAR', 'PKG', 'PH', 'PAYX', 'PAYC', 'PYPL', 'PENN', 'PNR', 'PBCT', 'PEP', 'PKI', 'PFE', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'POOL', 'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PTC', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'QCOM', 'PWR', 'DGX', 'RL', 'RJF', 'RTX', 'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RHI', 'ROK', 'ROL', 'ROP', 'ROST', 'RCL', 'SPGI', 'CRM', 'SBAC', 'SLB', 'STX', 'SEE', 'SRE', 'NOW', 'SHW', 'SPG', 'SWKS', 'SNA', 'SO', 'LUV', 'SWK', 'SBUX', 'STT', 'STE', 'SYK', 'SIVB', 'SYF', 'SNPS', 'SYY', 'TMUS', 'TROW', 'TTWO', 'TPR', 'TGT', 'TEL', 'TDY', 'TFX', 'TER', 'TSLA', 'TXN', 'TXT', 'COO', 'HIG', 'HSY', 'MOS', 'TRV', 'DIS', 'TMO', 'TJX', 'TSCO', 'TT', 'TDG', 'TRMB', 'TFC', 'TWTR', 'TYL', 'TSN', 'USB', 'UDR', 'ULTA', 'UAA', 'UA', 'UNP', 'UAL', 'UPS', 'URI', 'UNH', 'UHS', 'VLO', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VFC', 'VIAC', 'VTRS', 'V', 'VNO', 'VMC', 'WRB', 'GWW', 'WAB', 'WBA', 'WMT', 'WM', 'WAT', 'WEC', 'WFC', 'WELL', 'WST', 'WDC', 'WU', 'WRK', 'WY', 'WHR', 'WMB', 'WLTW', 'WYNN', 'XEL', 'XLNX', 'XYL', 'YUM', 'ZBRA', 'ZBH', 'ZION', 'ZTS']\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file containing the constituents information\n",
    "constituents = pd.read_csv(\"https://datahub.io/core/s-and-p-500-companies/r/constituents.csv\")\n",
    "\n",
    "# Get the ticker symbol for each constituent\n",
    "tickers = constituents['Symbol'].tolist()\n",
    "\n",
    "# Print the list of constituents\n",
    "print(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc8ec1-8e6f-41f7-a17c-208d4a9fb1c0",
   "metadata": {},
   "source": [
    "#### Download Returns for all tickers From YFinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1059bc-8081-4943-9a59-c828e8add940",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns=[]   \n",
    "for firm in tickers:\n",
    "    df_firm = yf.download(tickers=[firm], start=\"1990-01-01\")\n",
    "    df_firm[\"Return\"] = df_firm[\"Close\"].pct_change()\n",
    "    df_firm = df_firm[[\"Return\"]]\n",
    "    df_firm = df_firm.rename(columns={\"Return\": f\"{firm}\"})\n",
    "    returns.append(df_firm)\n",
    "df = pd.concat(returns, axis=1, sort=False)\n",
    "df = df.dropna(how='all',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4b47d583-b482-4c16-a9e0-2372344a386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df = df.copy()\n",
    "archive_df.to_csv(dest_dir+\"\\\\raw_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a53da-73ca-4f4c-a4d6-8b0b37f8ec2d",
   "metadata": {},
   "source": [
    "### Functions to Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4262806b-6f40-4acc-a592-ea8e5bf6b96b",
   "metadata": {},
   "source": [
    "#### Create Dataset function\n",
    "\n",
    "This function is used to creating initial dataset for financial time series analysis. The function takes in a dataframe containing stock market data, a split bucket number, and a destination directory dest_dir.\n",
    "\n",
    "The first step in the function is to select only the companies that existed at the beginning of the testing period. To do this, the function checks which companies were present on the 750th day of the split which is the first day for the testing period. It then selects only the columns of the dataframe corresponding to these companies.\n",
    "\n",
    "Next, the function calculates the target dataframe and after that we normalize the data.\n",
    "\n",
    "Then, we further create slices for test and training data using the below mentioned functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d897682-4bc9-4ec1-91d4-178491bf978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df_, sp, dest_dir): \n",
    "    '''\n",
    "        Select only the companies that existed at the beginning of testing period\n",
    "        In a split of 1000 days, we are checking the companies that were present on the 750th day\n",
    "        Then we are only using those companies\n",
    "    '''\n",
    "    \n",
    "    cols = df_.iloc[750].dropna().index.values # Columns on 750th date of the split\n",
    "    df_X = df_[cols] #Selecting only those columns\n",
    "    target_df = calculate_target_df(df_X)\n",
    "    normalized_df = normalize_df(df_X)\n",
    "    slice_train_dataset(normalized_df[:750], target_df[:750], sp=sp)\n",
    "    slice_test_dataset(normalized_df[750-240:],target_df[750-240:], dest_dir, sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d12620-89bb-4a3a-9d2e-536b6ab9704e",
   "metadata": {},
   "source": [
    "#### Calculate Target_Df (Y) function (Labeling)\n",
    "\n",
    "This function is used to calculate the target dataframe, which is used for training the financial time series classification model. The function takes in a dataframe df containing stock market data.\n",
    "\n",
    "Next, the function calculates the median return for each row in the prepared target dataframe. This median value is used as a threshold for classifying the returns as positive or negative.\n",
    "\n",
    "Finally, the function creates a target dataframe with the same dimensions as the prepared target dataframe, where stock returns that are above the daily median are labeled as one, and zero otherwise.\n",
    "\n",
    "The resulting target dataframe is used in the financial time series classification model to predict whether the stock returns will be positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe5d06-3c0a-4d04-bcdb-bcbc2f2692b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_target_df(df):\n",
    "    ''' \n",
    "        Stock returns that are above the daily median are labeled as one, and zero otherwise.\n",
    "        Returns a dataframe with the classification labels.\n",
    "    '''\n",
    "    df = prepare_target_df(df)\n",
    "    \n",
    "    # Calculate the median for each row\n",
    "    median = df.loc[:, :].median(axis=1)\n",
    "    \n",
    "    # Create the target dataframe\n",
    "    target_df = pd.DataFrame(0, index=df.index, columns=df.columns)\n",
    "    target_df[df.sub(median, axis=0).ge(0)] = 1\n",
    "    \n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ace68-1665-439a-83cd-09bf3722a820",
   "metadata": {},
   "source": [
    "#### Prepare Target_Df (Y) function\n",
    "\n",
    "This function is used to prepare the target dataframe by cleaning the input dataframe df.\n",
    "\n",
    "The function takes the input dataframe df, which contains stock market data, and creates a copy of it called copy_of_df. The reason for creating a copy is to avoid modifying the original dataframe.\n",
    "\n",
    "The function then iterates over the columns in the input dataframe and over the rows from the 240th row until the end of the dataframe. For each row, the function checks whether the previous 240 rows (i.e., the previous 240 days) for that column contain any missing values (NaN) in original dataframe. If there are missing values, it means that there isn't enough history to calculate the returns, so the function replaces the return value for that row and column with NaN in the copy_of_df.\n",
    "\n",
    "The function returns the copy_of_df dataframe with NaN values where there isn't enough history to calculate returns. This cleaned dataframe is then used to calculate the target dataframe in the calculate_target_df function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01871f06-15d4-4a78-aabf-50cd36784061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target_df(df):\n",
    "    ''' \n",
    "        Clean dataframe to create targets. \n",
    "        Remove any returns that don't have enough history so they don't count towards the labeling.\n",
    "    '''\n",
    "    copy_of_df = df.copy()\n",
    "    for cols in df.columns:\n",
    "        for i in range(240, len(df)):\n",
    "            if df[cols].iloc[i-240:i].isnull().values.any():\n",
    "                copy_of_df.iloc[i][cols] = np.nan\n",
    "    return copy_of_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327b25c-eea5-46a1-902b-2cb81264a99a",
   "metadata": {},
   "source": [
    "#### Normalising original dataframe (To be used as X in training data)\n",
    "\n",
    "The normalize_df function takes a pandas DataFrame df as an input and returns a normalized version of it.\n",
    "\n",
    "The normalization process involves subtracting the mean of the first 750 rows of the DataFrame (df.iloc[:750].mean()) from the entire DataFrame df, and then dividing the result by the standard deviation of the first 750 rows of the DataFrame (df.iloc[:750].std()). This effectively standardizes the data in the DataFrame, scaling it so that it has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "The function then returns the normalized DataFrame.\n",
    "\n",
    "#### Why do this?\n",
    "Normalizing stock returns data using mean and standard deviation is a common technique in finance and statistics to remove the scale and capture the relative variation of the data.\n",
    "\n",
    "Stock returns data often has large variations over time, and different stocks can have different levels of volatility. Normalizing the data using the mean and standard deviation allows us to standardize the returns data so that it can be compared across different stocks and time periods.\n",
    "\n",
    "By subtracting the mean from each data point, we center the data around 0, which removes any inherent bias in the data. Then, by dividing by the standard deviation, we scale the data to have a standard deviation of 1. This is useful because it allows us to compare the volatility of different stocks and time periods on a relative basis.\n",
    "\n",
    "Normalizing stock returns data can be particularly useful in financial modeling, such as in portfolio optimization or risk management, where we may want to analyze the risk and return characteristics of a portfolio of stocks or securities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daca3aa-cb4a-45d9-9876-03b7a5f500be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df):\n",
    "    mean_ = df.iloc[:750].mean()\n",
    "    std_ = df.iloc[:750].std()\n",
    "    return (df - mean_) / std_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668ca35-42b5-49cc-a92b-e311b3035f53",
   "metadata": {},
   "source": [
    "#### Slicing for training dataset\n",
    "\n",
    "The slice_train_dataset function takes two pandas DataFrames, df_X and df_target, as inputs, and a few optional parameters.\n",
    "\n",
    "The function slices the input data into shorter windows of length 240, and creates a training dataset by concatenating these windows. For each window, the function takes a subset of the columns in df_X and the corresponding target values in df_target.\n",
    "\n",
    "'sp' is a label that is used to identify the dataset in the saved files.\n",
    "\n",
    "Here's a step-by-step breakdown of what the function does:\n",
    "\n",
    "1. Create a list of column names cols from the columns in df_X.\n",
    "2. Initialize empty lists X_list and Y_list.\n",
    "3. Loop through each row in df_X, up to len(df_X)-240.\n",
    "4. For each row and column, extract a window of length 240 from df_X starting at the current row and append it to X_list.\n",
    "5. For the same row and column, extract the corresponding target value from df_target and append it to Y_list.\n",
    "6. If any values in the window or target are NaN, skip this row and column.\n",
    "7. After looping through all rows and columns, convert X_list and Y_list to NumPy arrays with the correct shape for training: X_train is a 3D array with shape (n_samples, window_length, n_features=1) and Y_train is a 2D array with shape (n_samples, 1).\n",
    "8. Save the resulting X_train and Y_train arrays as NumPy binary files with labels based on the sp parameter.\n",
    "\n",
    "In summary, this function prepares a dataset for training a machine learning model on a time series problem, where the goal is to predict future values of a target variable based on a window of past values. The function creates a training set by sliding a window of length 240 along the time series, and extracting a subset of columns from df_X and their corresponding target values from df_target. The function also saves the resulting dataset to disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8202fe20-279f-4ea4-b2d6-15a37305460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_train_dataset(df_X, df_target, sp=None):\n",
    "    cols = df_X.columns\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    for i in range(len(df_X)-240):\n",
    "        for col in cols:\n",
    "            X = df_X[col][i:i+240].values\n",
    "            Y = df_target[col][i+240]\n",
    "            if np.isnan(X).any() or np.isnan(Y):\n",
    "                continue\n",
    "            else:\n",
    "                X_list.append(X)\n",
    "                Y_list.append(Y)\n",
    "    X_train = np.array(X_list).reshape(-1,240,1)\n",
    "    Y_train = np.array(Y_list).reshape(-1,1) \n",
    "    np.save(op.join(dest_dir, 'study_period_X_'+str(sp)+'_train.npy'), X_train)\n",
    "    np.save(op.join(dest_dir, 'study_period_Y_'+str(sp)+'_train.npy'), Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef4ea5-0483-411b-b416-19e50517c9c0",
   "metadata": {},
   "source": [
    "#### Slicing for test dataset\n",
    "\n",
    "The slice_test_dataset function takes two pandas DataFrames, df_X and df_target, and two additional parameters dest_dir and sp, as inputs. The function is similar to slice_train_dataset but prepares a dataset for testing a machine learning model on a time series problem.\n",
    "\n",
    "The function first creates a list of column names cols from the columns in df_X. It then initializes empty lists index_list, dates, X_list, and Y_list. index_list keeps track of the row and column index for each sample in the test dataset, and dates keeps track of the date corresponding to each row.\n",
    "\n",
    "The function loops through each row in df_X, starting at index lookback, which is set to 240. For each row and column, the function extracts a window of length 240 from df_X ending at the current row, and appends it to X_list. It also extracts the corresponding target value from df_target and appends it to Y_list. The function also adds the row and column index to index_list and the date to dates.\n",
    "\n",
    "If any values in the window or target are NaN, the function skips this row and column. After looping through all rows and columns, the function converts X_list and Y_list to NumPy arrays with the correct shape for testing: X_test is a 3D array with shape (n_samples, window_length, n_features=1) and Y_test is a 2D array with shape (n_samples, 1).\n",
    "\n",
    "The function also saves the resulting X_test and Y_test arrays, as well as the columns, dates, and index_array to disk. These files are saved in a directory named sp{sp} under the directory specified by dest_dir.\n",
    "\n",
    "In summary, this function prepares a dataset for testing a machine learning model on a time series problem, where the goal is to evaluate the model's performance on a hold-out set of data. The function creates a testing set by sliding a window of length 240 along the time series, and extracting a subset of columns from df_X and their corresponding target values from df_target. The function also saves the resulting dataset and related metadata to disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6c3d8957-5603-4779-a510-a13f4ebadfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_test_dataset(df_X, df_target, dest_dir, sp):\n",
    "    cols = df_X.columns\n",
    "    index_list, dates = [], []\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    lookback = 240\n",
    "    for i in range(lookback, len(df_X)):\n",
    "        dates.append(df_X.index[i])\n",
    "        for j,col in enumerate(cols):\n",
    "            X = df_X[col][i-lookback:i].values\n",
    "            Y = df_target[col][i]\n",
    "            if np.isnan(X).any() or np.isnan(df_X[col].iloc[i]):\n",
    "                continue\n",
    "            else: \n",
    "                index_list.append([i-240, j])\n",
    "                X_list.append(X)\n",
    "                Y_list.append(Y)\n",
    "    columns = np.array(df_X.columns)\n",
    "    dates_array = np.array(dates)\n",
    "    index_array = np.array(index_list)\n",
    "    inference_dir = op.join(dest_dir, 'sp'+str(sp))\n",
    "    X_test = np.array(X_list).reshape(-1,240,1)\n",
    "    Y_test = np.array(Y_list).reshape(-1,1)\n",
    "    create_directory(inference_dir)\n",
    "    np.save(op.join(inference_dir, 'columns.npy'), columns)\n",
    "    np.save(op.join(inference_dir, 'dates.npy'), dates_array)\n",
    "    np.save(op.join(inference_dir, 'index_array.npy'), index_array)\n",
    "    np.save(op.join(dest_dir, 'study_period_X_'+str(sp)+'_test.npy'), X_test)\n",
    "    np.save(op.join(dest_dir, 'study_period_Y_'+str(sp)+'_test.npy'), Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86149a39-40ef-438c-84a3-4ab946b90e09",
   "metadata": {},
   "source": [
    "### Splitting in small buckets\n",
    "Here we are running the pre-processing code. We are dividing the data into small stocks into small buckets of 1000 days and applying different data augmentation methods to each bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383250cd-02ba-46a0-80a2-10849e24a109",
   "metadata": {},
   "source": [
    "#### Why do this?: Data Sparsity problems\n",
    "In her paper titled \"Evaluating Data Augmentation for Financial Time Series Classification\" (Fons, 2020), Elizabeth Fons proposed a novel approach to address the data sparsity problem for individual stocks in financial time series data by dividing the S&P 500 stocks into small buckets of 1000 days and applying different data augmentation methods to each bucket.\n",
    "\n",
    "The data sparsity problem arises due to the varying lengths of historical data available for each individual stock. To address this issue, we group stocks with similar amounts of historical data together into small buckets of 1000 days. By doing so, we ensure that each bucket contains a comparable amount of data for each stock, which makes it possible to apply data augmentation methods to each bucket separately.\n",
    "\n",
    "This approach allows us to augment the data within each bucket using a variety of methods, such as random rotation, cropping, flipping, and scaling. By doing so, we can generate more training data for each stock and improve the performance of machine learning models that are trained on this data.\n",
    "\n",
    "Overall, this method represents a practical approach to dealing with data sparsity in individual stocks and applying data augmentation methods effectively. By dividing the data into small buckets and applying different data augmentation methods to each bucket, we are able to mitigate the effects of data sparsity and generate more robust training data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ee7b1f33-ba07-4662-9044-b9edbcbc836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dest_dir, df):\n",
    "    j = 0\n",
    "    count = 0\n",
    "    while count+1000 < len(df):\n",
    "        print(\"Split :\"+str(j+1))\n",
    "        df_ = df.iloc[count:count+1000]\n",
    "        create_dataset(df_, j, dest_dir)\n",
    "        count += 250\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddfc6cf-bd67-40df-987e-7f4d5d048d39",
   "metadata": {},
   "source": [
    "### Bringing everything together\n",
    "Running the pre-processing code. It will take a lot of time. Total size on the disk ~ 15 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e81d746b-f23d-4abf-a2f9-30a414826b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split :0\n",
      "Split :1\n",
      "Split :2\n",
      "Split :3\n",
      "Split :4\n",
      "Split :5\n",
      "Split :6\n",
      "Split :7\n",
      "Split :8\n",
      "Split :9\n",
      "Split :10\n",
      "Split :11\n",
      "Split :12\n",
      "Split :13\n",
      "Split :14\n",
      "Split :15\n",
      "Split :16\n",
      "Split :17\n",
      "Split :18\n",
      "Split :19\n",
      "Split :20\n",
      "Split :21\n",
      "Split :22\n",
      "Split :23\n",
      "Split :24\n",
      "Split :25\n",
      "Split :26\n",
      "Split :27\n",
      "Split :28\n",
      "Split :29\n"
     ]
    }
   ],
   "source": [
    "process_dataset(dest_dir, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10388cd5-a3e6-4d56-ae80-545197c6f85a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
